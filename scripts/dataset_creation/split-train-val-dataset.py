#!/usr/bin/env python3
"""
è‡ªåŠ¨åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„è„šæœ¬

åŠŸèƒ½ï¼š
1. ä»è®­ç»ƒé›†ä¸­éšæœºæŠ½å–æŒ‡å®šæ¯”ä¾‹æˆ–æ•°é‡çš„æ ·æœ¬ä½œä¸ºéªŒè¯é›†
2. è‡ªåŠ¨å¤åˆ¶å¯¹åº”çš„è§†é¢‘ã€maskå’Œå…ƒæ•°æ®æ–‡ä»¶
3. ä»åŸè®­ç»ƒé›†ä¸­ç§»é™¤éªŒè¯é›†æ ·æœ¬ï¼ˆå¯é€‰ï¼‰
4. ç”Ÿæˆè¯¦ç»†çš„åˆ’åˆ†æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    # æŒ‰æ¯”ä¾‹åˆ’åˆ†ï¼ˆé»˜è®¤10%ï¼‰
    python split-train-val-dataset.py --src-root /root/autodl-tmp/train_with_mosaic
    
    # æŒ‰å›ºå®šæ•°é‡åˆ’åˆ†
    python split-train-val-dataset.py --src-root /root/autodl-tmp/train_with_mosaic --val-size 500
    
    # ä¸ä»è®­ç»ƒé›†åˆ é™¤ï¼ˆä»…å¤åˆ¶ï¼‰
    python split-train-val-dataset.py --src-root /root/autodl-tmp/train_with_mosaic --no-remove
"""

import argparse
import json
import random
import shutil
from pathlib import Path
from typing import List, Dict, Tuple
import sys


def parse_args():
    parser = argparse.ArgumentParser(
        description='Split mosaic restoration dataset into train and validation sets'
    )
    parser.add_argument(
        '--src-root',
        type=str,
        required=True,
        help='Source dataset root directory (e.g., /root/autodl-tmp/train_with_mosaic)'
    )
    parser.add_argument(
        '--dst-root',
        type=str,
        default=None,
        help='Destination validation set root directory (default: {src-root}_val)'
    )
    parser.add_argument(
        '--val-ratio',
        type=float,
        default=0.1,
        help='Validation set ratio (default: 0.1, i.e., 10%%)'
    )
    parser.add_argument(
        '--val-size',
        type=int,
        default=None,
        help='Fixed validation set size (overrides --val-ratio if specified)'
    )
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='Random seed for reproducibility (default: 42)'
    )
    parser.add_argument(
        '--no-remove',
        action='store_true',
        help='Do not remove validation samples from training set (only copy)'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Preview what would be done without actually copying/moving files'
    )
    return parser.parse_args()


def collect_metadata_files(src_root: Path) -> List[Path]:
    """æ”¶é›†æ‰€æœ‰å…ƒæ•°æ®JSONæ–‡ä»¶"""
    meta_dir = src_root / 'crop_unscaled_meta'
    if not meta_dir.exists():
        raise FileNotFoundError(f"Metadata directory not found: {meta_dir}")
    
    meta_files = list(meta_dir.glob('*.json'))
    if not meta_files:
        raise FileNotFoundError(f"No JSON files found in {meta_dir}")
    
    return meta_files


def parse_metadata(meta_path: Path) -> Dict:
    """è§£æå…ƒæ•°æ®æ–‡ä»¶"""
    try:
        with open(meta_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"Warning: Failed to parse {meta_path}: {e}")
        return None


def resolve_file_paths(meta_path: Path, metadata: Dict) -> Tuple[Path, Path]:
    """è§£æè§†é¢‘å’Œmaskæ–‡ä»¶çš„ç»å¯¹è·¯å¾„"""
    base_dir = meta_path.parent
    
    # è§£æç›¸å¯¹è·¯å¾„
    video_rel = metadata.get('relative_nsfw_video_path', '')
    mask_rel = metadata.get('relative_mask_video_path', '')
    
    if not video_rel or not mask_rel:
        raise ValueError(f"Missing video/mask paths in {meta_path.name}")
    
    # æ„å»ºç»å¯¹è·¯å¾„
    video_path = (base_dir / video_rel).resolve()
    mask_path = (base_dir / mask_rel).resolve()
    
    return video_path, mask_path


def create_val_directory_structure(dst_root: Path):
    """åˆ›å»ºéªŒè¯é›†ç›®å½•ç»“æ„"""
    for subdir in ['crop_unscaled_img', 'crop_unscaled_mask', 'crop_unscaled_meta']:
        (dst_root / subdir).mkdir(parents=True, exist_ok=True)


def copy_sample(meta_path: Path, video_path: Path, mask_path: Path, 
                dst_root: Path, dry_run: bool = False) -> bool:
    """å¤åˆ¶ä¸€ä¸ªæ ·æœ¬åˆ°éªŒè¯é›†"""
    try:
        dst_meta = dst_root / 'crop_unscaled_meta' / meta_path.name
        dst_video = dst_root / 'crop_unscaled_img' / video_path.name
        dst_mask = dst_root / 'crop_unscaled_mask' / mask_path.name
        
        if dry_run:
            print(f"  [DRY-RUN] Would copy:")
            print(f"    {meta_path} -> {dst_meta}")
            print(f"    {video_path} -> {dst_video}")
            print(f"    {mask_path} -> {dst_mask}")
            return True
        
        # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not video_path.exists():
            print(f"  Warning: Video file not found: {video_path}")
            return False
        if not mask_path.exists():
            print(f"  Warning: Mask file not found: {mask_path}")
            return False
        
        # å¤åˆ¶æ–‡ä»¶
        shutil.copy2(meta_path, dst_meta)
        shutil.copy2(video_path, dst_video)
        shutil.copy2(mask_path, dst_mask)
        
        return True
    except Exception as e:
        print(f"  Error copying sample {meta_path.name}: {e}")
        return False


def remove_sample(meta_path: Path, video_path: Path, mask_path: Path, 
                  dry_run: bool = False) -> bool:
    """ä»è®­ç»ƒé›†ä¸­åˆ é™¤æ ·æœ¬"""
    try:
        if dry_run:
            print(f"  [DRY-RUN] Would remove:")
            print(f"    {meta_path}")
            print(f"    {video_path}")
            print(f"    {mask_path}")
            return True
        
        meta_path.unlink(missing_ok=True)
        video_path.unlink(missing_ok=True)
        mask_path.unlink(missing_ok=True)
        
        return True
    except Exception as e:
        print(f"  Error removing sample {meta_path.name}: {e}")
        return False


def main():
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    
    # è·¯å¾„è®¾ç½®
    src_root = Path(args.src_root)
    dst_root = Path(args.dst_root) if args.dst_root else Path(str(src_root) + '_val')
    
    print("=" * 80)
    print("è®­ç»ƒé›†/éªŒè¯é›†åˆ’åˆ†å·¥å…·")
    print("=" * 80)
    print(f"æºè®­ç»ƒé›†è·¯å¾„: {src_root}")
    print(f"ç›®æ ‡éªŒè¯é›†è·¯å¾„: {dst_root}")
    print(f"éšæœºç§å­: {args.seed}")
    
    # æ”¶é›†å…ƒæ•°æ®æ–‡ä»¶
    print("\n[1/5] æ”¶é›†å…ƒæ•°æ®æ–‡ä»¶...")
    meta_files = collect_metadata_files(src_root)
    total_samples = len(meta_files)
    print(f"  æ‰¾åˆ° {total_samples} ä¸ªæ ·æœ¬")
    
    # ç¡®å®šéªŒè¯é›†å¤§å°
    if args.val_size is not None:
        val_size = min(args.val_size, total_samples)
        print(f"\n[2/5] ä½¿ç”¨å›ºå®šéªŒè¯é›†å¤§å°: {val_size} ä¸ªæ ·æœ¬")
    else:
        val_size = max(1, int(total_samples * args.val_ratio))
        print(f"\n[2/5] ä½¿ç”¨éªŒè¯é›†æ¯”ä¾‹: {args.val_ratio * 100:.1f}% = {val_size} ä¸ªæ ·æœ¬")
    
    train_size = total_samples - val_size
    print(f"  è®­ç»ƒé›†: {train_size} ä¸ªæ ·æœ¬ ({train_size/total_samples*100:.1f}%)")
    print(f"  éªŒè¯é›†: {val_size} ä¸ªæ ·æœ¬ ({val_size/total_samples*100:.1f}%)")
    
    # éšæœºæŠ½æ ·
    print(f"\n[3/5] éšæœºæŠ½æ · {val_size} ä¸ªæ ·æœ¬...")
    val_samples = random.sample(meta_files, val_size)
    print(f"  å®ŒæˆæŠ½æ ·")
    
    # åˆ›å»ºç›®æ ‡ç›®å½•
    if not args.dry_run:
        print(f"\n[4/5] åˆ›å»ºéªŒè¯é›†ç›®å½•ç»“æ„...")
        create_val_directory_structure(dst_root)
        print(f"  ç›®å½•ç»“æ„å·²åˆ›å»º")
    else:
        print(f"\n[4/5] [DRY-RUN] è·³è¿‡ç›®å½•åˆ›å»º")
    
    # å¤åˆ¶æ ·æœ¬
    print(f"\n[5/5] å¤åˆ¶æ ·æœ¬åˆ°éªŒè¯é›†...")
    success_count = 0
    failed_samples = []
    
    for i, meta_path in enumerate(val_samples, 1):
        if i % 100 == 0 or i == 1:
            print(f"  å¤„ç†è¿›åº¦: {i}/{val_size}")
        
        # è§£æå…ƒæ•°æ®
        metadata = parse_metadata(meta_path)
        if metadata is None:
            failed_samples.append(meta_path.name)
            continue
        
        try:
            # è·å–æ–‡ä»¶è·¯å¾„
            video_path, mask_path = resolve_file_paths(meta_path, metadata)
            
            # å¤åˆ¶åˆ°éªŒè¯é›†
            if copy_sample(meta_path, video_path, mask_path, dst_root, args.dry_run):
                success_count += 1
                
                # ä»è®­ç»ƒé›†åˆ é™¤
                if not args.no_remove:
                    remove_sample(meta_path, video_path, mask_path, args.dry_run)
            else:
                failed_samples.append(meta_path.name)
                
        except Exception as e:
            print(f"  Error processing {meta_path.name}: {e}")
            failed_samples.append(meta_path.name)
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "=" * 80)
    print("åˆ’åˆ†å®Œæˆï¼")
    print("=" * 80)
    
    if args.dry_run:
        print("\nâš ï¸  è¿™æ˜¯ä¸€æ¬¡æ¨¡æ‹Ÿè¿è¡Œï¼Œæ²¡æœ‰å®é™…ä¿®æ”¹ä»»ä½•æ–‡ä»¶")
    
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  åŸå§‹è®­ç»ƒé›†æ ·æœ¬æ•°: {total_samples}")
    print(f"  æˆåŠŸå¤åˆ¶åˆ°éªŒè¯é›†: {success_count}")
    print(f"  å¤åˆ¶å¤±è´¥: {len(failed_samples)}")
    
    if not args.no_remove and not args.dry_run:
        remaining = len(list((src_root / 'crop_unscaled_meta').glob('*.json')))
        print(f"  è®­ç»ƒé›†å‰©ä½™æ ·æœ¬æ•°: {remaining}")
    
    print(f"\nğŸ“ æ–‡ä»¶ä½ç½®:")
    print(f"  è®­ç»ƒé›†: {src_root}")
    print(f"  éªŒè¯é›†: {dst_root}")
    
    if failed_samples:
        print(f"\nâš ï¸  å¤±è´¥çš„æ ·æœ¬ ({len(failed_samples)}):")
        for name in failed_samples[:10]:
            print(f"    - {name}")
        if len(failed_samples) > 10:
            print(f"    ... è¿˜æœ‰ {len(failed_samples) - 10} ä¸ª")
    
    print(f"\nâœ… ä¸‹ä¸€æ­¥:")
    print(f"  1. æ£€æŸ¥éªŒè¯é›†ç›®å½•: {dst_root}")
    print(f"  2. æ›´æ–°è®­ç»ƒé…ç½®æ–‡ä»¶ä¸­çš„ val_dataloader.dataset.metadata_root_dir")
    print(f"  3. é‡æ–°å¯åŠ¨è®­ç»ƒä»¥ä½¿ç”¨æ–°çš„éªŒè¯é›†")
    
    if args.dry_run:
        print(f"\nğŸ’¡ å¦‚æœé¢„è§ˆç»“æœæ­£ç¡®ï¼Œè¯·ç§»é™¤ --dry-run å‚æ•°é‡æ–°è¿è¡Œ")
    
    print("\n" + "=" * 80)


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\næ“ä½œå·²å–æ¶ˆ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

